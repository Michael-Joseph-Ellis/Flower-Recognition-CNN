{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Flower Recognition CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wvnC7w920xB0",
        "outputId": "b98badaa-1798-4f56-ce66-942bee3b5063"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd drive/MyDrive\n",
        "!ls \n",
        "!unzip = archive.zip # my zip file name "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Packages and Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PgqKG-Stz1GI"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random\n",
        "import os\n",
        "import cv2\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.optimizers.legacy import Adam\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Classification\n",
        "As well as intializing the working directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lICkD21dz1GM"
      },
      "outputs": [],
      "source": [
        "# Categories of flowers we want to classify\n",
        "categories = ['dandelion', 'daisy', 'sunflower', 'tulip', 'rose']\n",
        "\n",
        "# Data directory\n",
        "data_directory = '/content/drive/MyDrive/flowers'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Loading and iterating the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBcOzdHSz1GN"
      },
      "outputs": [],
      "source": [
        "# Load and preprocess the image data\n",
        "features = []\n",
        "# Iterate through each category and assign a class index to each\n",
        "for category in categories:\n",
        "    path = os.path.join(data_directory, category)\n",
        "    class_index = categories.index(category)\n",
        "    # Iterate through each image in the data directory\n",
        "    for img in os.listdir(path):\n",
        "        if img.endswith('.jpg'):\n",
        "            img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_COLOR)\n",
        "            img_array = cv2.resize(img_array, (150, 150))\n",
        "            features.append([img_array, class_index])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Checking path existence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_tbI2U26z1GO"
      },
      "outputs": [],
      "source": [
        "'''Kind of redundant stuff I realize I added for no reason, but I'm keeping it here for now just in case I need it later.\n",
        "This code would basically check to see if the path existed and do the same thing as the code above. At this point this is basically just filler code.'''\n",
        "\n",
        "# Specify paths without leading slash\n",
        "dandelion_path = '/content/drive/MyDrive/flowers/dandelion'\n",
        "\n",
        "# Check if the directory exists\n",
        "if os.path.exists(dandelion_path):\n",
        "    class_index = categories.index(category)\n",
        "    for img in os.listdir(dandelion_path):\n",
        "        if img.endswith('.jpg'):\n",
        "            img_array = cv2.imread(os.path.join(dandelion_path, img), cv2.IMREAD_COLOR)\n",
        "else:\n",
        "    print(f\"The directory {dandelion_path} does not exist.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Preprocessing\n",
        "\n",
        "Separating the features and labels from the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6rvNB9uz1GO"
      },
      "outputs": [],
      "source": [
        "X = []\n",
        "y = []\n",
        "# Iterate through each image in the data directory\n",
        "for img, label in features:\n",
        "    X.append(img)\n",
        "    y.append(label)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reshape and normalize the image data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4z_6uyf-z1GP"
      },
      "outputs": [],
      "source": [
        "# Convert the lists to NumPy arrays and normalize the image data\n",
        "X = np.array(X) / 255.0\n",
        "y = np.array(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data visualizations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaKezE88z1GP"
      },
      "outputs": [],
      "source": [
        "# Visualize some random images from the dataset\n",
        "fig, ax = plt.subplots(2, 5)\n",
        "fig.set_size_inches(30, 15)\n",
        "# Plot 10 images\n",
        "for i in range(2):\n",
        "    for j in range(5):\n",
        "        index = random.randint(0, len(X))\n",
        "        ax[i, j].imshow(X[index])\n",
        "        ax[i, j].set_title('Flower: ' + categories[y[index]], fontsize=15)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Class labels are distributed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Shn29rZZz1GQ"
      },
      "outputs": [],
      "source": [
        "# Class labels distributed\n",
        "# Explore class distribution\n",
        "plt.figure(figsize=(17, 10))\n",
        "sns.countplot(x=[categories[i] for i in y])\n",
        "plt.xticks(rotation=45, fontsize=20)\n",
        "plt.title(\"Class Distribution\", fontsize=22)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check for noises"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eJdjGzcwz1GQ"
      },
      "outputs": [],
      "source": [
        "''' Don't know the actual point of this code, but in relation to CNN's, they are used for preprocessing and quality control. Overall for me this also feels like filler.'''\n",
        "\n",
        "# Check for image noise and print results\n",
        "def check_for_noise(image, noise_threshold=30):\n",
        "    # Ensure the image is in 8-bit format\n",
        "    image = cv2.convertScaleAbs(image)\n",
        "\n",
        "    # Convert the image to grayscale for noise analysis\n",
        "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "    # Calculate the standard deviation of pixel intensities in the grayscale image\n",
        "    std_dev = np.std(gray_image)\n",
        "\n",
        "    # Check if the standard deviation exceeds the noise threshold\n",
        "    return std_dev > noise_threshold\n",
        "\n",
        "all_noise_free = True\n",
        "\n",
        "for i, img in enumerate(X):\n",
        "    is_noisy = check_for_noise(img)\n",
        "    if is_noisy:\n",
        "        print(f\"Image {i + 1} has noise.\")\n",
        "        all_noise_free = False\n",
        "\n",
        "if all_noise_free:\n",
        "    print(\"All images are noise-free.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Balancing the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9nLohIRbz1GR"
      },
      "outputs": [],
      "source": [
        "''' This code balances the class distribution by undersampling the majority of classes randomly to match the count of the minority classes. \n",
        "    This is commonly done to prevent the model from being biased towards the majority class during training. \n",
        "    Balancing the dataset can be particularly important when dealing with imbalanced classes in machine learning tasks. '''\n",
        "\n",
        "# Count the number of samples in each class\n",
        "class_counts = dict(zip(*np.unique(y, return_counts=True)))\n",
        "\n",
        "# Find the class with the fewest samples\n",
        "min_class = min(class_counts, key=class_counts.get)\n",
        "min_count = class_counts[min_class]\n",
        "\n",
        "# Perform random undersampling of the majority classes\n",
        "X_balanced = []\n",
        "y_balanced = []\n",
        "\n",
        "# Ensure that at least one sample from the class with the fewest instances is included\n",
        "for cls in class_counts:\n",
        "    X_majority = X[y == cls]\n",
        "\n",
        "    if cls == min_class:\n",
        "        X_majority_sampled = X_majority\n",
        "    else:\n",
        "        X_majority_sampled = resample(X_majority, n_samples=min_count, random_state=42)\n",
        "\n",
        "    X_balanced.append(X_majority_sampled)\n",
        "    y_balanced.extend([cls] * len(X_majority_sampled))\n",
        "\n",
        "X_balanced = np.vstack(X_balanced)\n",
        "y_balanced = np.array(y_balanced)\n",
        "\n",
        "# Check the class distribution after undersampling\n",
        "class_counts = Counter(y_balanced)\n",
        "\n",
        "# Convert the Counter object to a list of counts\n",
        "class_counts_list = [class_counts[class_label] for class_label in sorted(class_counts.keys())]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xrZkgq6zz1GS"
      },
      "outputs": [],
      "source": [
        "# Visualize the class distribution after balancing\n",
        "sns.set_style('whitegrid')\n",
        "plt.figure(figsize=(18, 10))\n",
        "plt.pie(class_counts_list, labels=categories, startangle=90, colors=['r', 'g', 'b', 'y', 'm'], autopct='%1.1f%%', explode=(0, 0.1, 0, 0, 0), shadow=True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Split the data into a training and test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qFV7SUFNz1GT"
      },
      "outputs": [],
      "source": [
        "# Split data into a training and set test\n",
        "X_balanced.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This part is responsible for splitting the balanced dataset into training and testing sets. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-OfCt_yCz1GT"
      },
      "outputs": [],
      "source": [
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
        "''' test_size = 0.2 means that 20% of the data will be used for testing and 80% for training. \n",
        "    random_state = 42 means that the data will be split in a random manner, but the random number generator will be seeded with the number 42.\n",
        "    This means that if you run the same code with the same random seed, you should get the same results. '''\n",
        "print(\"X_train shape :\",X_train.shape)\n",
        "print(\"y_train shape :\",y_train.shape)\n",
        "print(\"X_test shape :\" ,X_test.shape)\n",
        "print(\"y_test shape :\",y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KcEB2eKzz1GT"
      },
      "outputs": [],
      "source": [
        "# Optionally, you can one-hot encode the labels if you plan to use deep learning models but since this is a cnn...\n",
        "''' I don't really know what it means to one-hot encode the labels but I think it means to convert the labels into a binary matrix?\n",
        "    A use case for this is when youre dealing with categorical cross-entropy loss in neural networks. It provides a good way to represent categorical labels in a format that \n",
        "    is useful for neural networks. '''\n",
        "y_train = to_categorical(y_train, num_classes=len(categories))\n",
        "y_test = to_categorical(y_test, num_classes=len(categories))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "2baI76izz1GU"
      },
      "outputs": [],
      "source": [
        "''' This is where we train the model. \n",
        "    During training we use image augmentation to generate new images from the existing ones. \n",
        "    We do this by using the ImageDataGenerator class from Keras to provide the model with a stream of augmented images samples for training.'''\n",
        "\n",
        "model = keras.Sequential([\n",
        "    layers.Conv2D(32, (3, 3), input_shape=(150, 150, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Dropout(0.3),\n",
        "\n",
        "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
        "    layers.MaxPooling2D(pool_size=(2, 2)),\n",
        "    layers.Flatten(),\n",
        "    layers.Dense(512, activation=\"relu\"),\n",
        "    layers.Dropout(0.3),\n",
        "    layers.Dense(len(categories), activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Assuming your data is properly preprocessed, one-hot encoded, and loaded\n",
        "# X_train, y_train, X_test, y_test = ...\n",
        "\n",
        "datagen = ImageDataGenerator(rotation_range=20, width_shift_range=0.2, height_shift_range=0.2, horizontal_flip=True)\n",
        "datagen.fit(X_train)\n",
        "\n",
        "# Train the model\n",
        "history = model.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=30, batch_size=32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B6HEuZfUz1GU"
      },
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
        "print(f\"Test accuracy: {test_acc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8JVS8pnxz1GU"
      },
      "outputs": [],
      "source": [
        "# Plotting the training and validation loss\n",
        "\n",
        "plt.plot(history.history['accuracy'], label='Training accuracy')\n",
        "plt.plot(history.history['val_accuracy'], label='Validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EG2tCjFXz1GV"
      },
      "outputs": [],
      "source": [
        "# Make predictions on the test set\n",
        "predictions = model.predict(X_test)\n",
        "\n",
        "# Get the class labels for the predictions\n",
        "predicted_labels = np.argmax(predictions, axis=1)\n",
        "\n",
        "# Get the true class labels for the test set\n",
        "true_labels = np.argmax(y_test, axis=1)\n",
        "\n",
        "# Define a function to display images with labels\n",
        "def display_images_with_labels(images, true_labels, predicted_labels, class_names):\n",
        "    fig, axes = plt.subplots(5, 5, figsize=(15, 10))\n",
        "\n",
        "    # Increase the space between rows\n",
        "    fig.subplots_adjust(hspace=0.5)\n",
        "\n",
        "    for i, ax in enumerate(axes.flat):\n",
        "        ax.imshow(images[i])\n",
        "        ax.axis('off')\n",
        "        true_label = class_names[true_labels[i]]\n",
        "        predicted_label = class_names[predicted_labels[i]]\n",
        "        title = f\"True: {true_label}\\nPredicted: {predicted_label}\"\n",
        "        ax.set_title(title)\n",
        "\n",
        "# Assuming you have a list of class names\n",
        "class_names = ['dandelion', 'daisy', 'sunflower', 'tulip', 'rose']\n",
        "\n",
        "# Display a random sample of test images\n",
        "sample_indices = np.random.choice(len(X_test), 25, replace=False)\n",
        "sample_images = X_test[sample_indices]\n",
        "sample_true_labels = true_labels[sample_indices]\n",
        "sample_predicted_labels = predicted_labels[sample_indices]\n",
        "\n",
        "display_images_with_labels(sample_images, sample_true_labels, sample_predicted_labels, class_names)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YhQgK_j-z1GV"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score, recall_score,f1_score\n",
        "\n",
        "\n",
        "f1 = f1_score(true_labels, predicted_labels, average='micro')\n",
        "\n",
        "# Output the result\n",
        "print(\"F1 Score:\", f1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "datasetId": 8782,
          "sourceId": 2431805,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30579,
      "isGpuEnabled": false,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (mlbd)",
      "language": "python",
      "name": "mlbd"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
